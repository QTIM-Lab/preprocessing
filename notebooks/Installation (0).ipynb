{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3300ee-1d09-4a02-87a2-8ddb112b51c6",
   "metadata": {},
   "source": [
    "# Installation Guide\n",
    "\n",
    "This guide illustrates the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac6675-1dd1-4d9e-a593-e58572ce980b",
   "metadata": {},
   "source": [
    "## Local Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807a6fb-cef0-463f-a651-f7732ab6d6e7",
   "metadata": {},
   "source": [
    "### External Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60552621-5536-4e45-bd70-75512f484208",
   "metadata": {},
   "source": [
    "#### dcm2niix\n",
    "This tool is required for the conversion of DICOM files to the NIfTI format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfe1a2-1954-4c51-a1f9-ddaab9eab7af",
   "metadata": {},
   "source": [
    "### model weights\n",
    "In order to complete skullstripping and registration tasks, `preprocessing` relies on [SynthStrip](https://surfer.nmr.mgh.harvard.edu/docs/synthstrip/) and [SynthMorph](https://martinos.org/malte/synthmorph/). The first time a command that requires these models is called, you will be prompted to define an environment variable `PREPROCESSING_MODELS_PATH` and to update your RC file to be used in the future. If you are on a QTIM machine, these models are already downloaded and available if you specify 'QTIM'. If you wish to use `preprocessing` in a script of your own, it is recommended that you add this environment variable to your RC file ahead of local installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2a1a7-5510-4b78-ba0b-edf3d983fe97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### pip\n",
    "\n",
    "From the repo root, run \n",
    "```bash\n",
    "pip install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f8ff8-3be0-4afc-9eb3-4ba3d07298d1",
   "metadata": {},
   "source": [
    "### poetry\n",
    "\n",
    "Within the repository root directory, a poetry.lock file is already present. To install `preprocessing`, simply navigate to this directory, activate the desired virtual environment, and run\n",
    "\n",
    "```bash\n",
    "poetry install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7bd292-2fcd-4e37-80cf-e61a9504898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: \n",
      "preprocessing <command> [<args>]\n",
      "\n",
      "The following commands are available:\n",
      "    validate-installation       Check that the `preprocessing` library is installed correctly along\n",
      "                                with all of its dependencies.\n",
      "\n",
      "    dicom-dataset               Create a DICOM dataset CSV compatible with subsequent `preprocessing`\n",
      "                                scripts. The final CSV provides a series level summary of the location\n",
      "                                of each series alongside metadata extracted from DICOM headers.  If the\n",
      "                                previous organization schems of the dataset does not enforce a DICOM\n",
      "                                series being isolated to a unique directory (instances belonging to\n",
      "                                multiple series must not share the same lowest level directory),\n",
      "                                reorganization must be applied for NIfTI conversion.\n",
      "\n",
      "    nifti-dataset               Create a NIfTI dataset CSV compatible with subsequent `preprocessing`\n",
      "                                scripts. The final CSV provides a series level summary of the location\n",
      "                                of each series alongside metadata generated to simulate DICOM headers.\n",
      "                                Specifically, ['PatientID', 'StudyDate', 'SeriesInstanceUID',\n",
      "                                'SeriesDescription', 'StudyInstanceUID'] (and optionally\n",
      "                                'NormalizedSeriesDescription') are inferred or randomly generated.\n",
      "\n",
      "    dataset-to-nifti            Convert DICOMs to NIfTI file format. A CSV is required to map a\n",
      "                                DICOM series to the resulting .nii.gz file and to provide\n",
      "                                the context for filenames. The outputs will follow a BIDS inspired\n",
      "                                convention.\n",
      "\n",
      "    brain-preprocessing         Preprocess NIfTI files for deep learning. A CSV is required to\n",
      "                                indicate the location of source files and to procide the context\n",
      "                                for filenames. The outputs will follow a BIDS inspired convention.\n",
      "\n",
      "    track-tumors                Longitudinal tracking of individual tumors. Each connected component\n",
      "                                for a given label within a segmentation mask is assigned a unique ID\n",
      "                                that will remain consistent across all scans belonging to the same\n",
      "                                patient. This command assumes that longitudinal or atlas registration\n",
      "                                was used when preprocessing the data.\n",
      "\n",
      "Run `preprocessing <command> --help` for more details about how to use each individual command.\n",
      "\n",
      "positional arguments:\n",
      "  {validate-installation,dicom-dataset,nifti-dataset,dataset-to-nifti,brain-preprocessing,track-tumors,track-volume}\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!preprocessing --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61df60e-d7e2-4a80-94a3-83c6696e3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!preprocessing validate-installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ec779-e166-475c-b73e-da29a6824a3e",
   "metadata": {},
   "source": [
    "## docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199ebd1-f554-4a24-9472-1dd8771e03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!preprocessing-docker --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba168fd2-e757-40dd-b8b9-f10816d54413",
   "metadata": {},
   "outputs": [],
   "source": [
    "!preprocessing-docker validate-installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeabbf7c-92cb-4246-b03e-11c95ae70c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package preprocessing:\n",
      "\n",
      "NAME\n",
      "    preprocessing\n",
      "\n",
      "DESCRIPTION\n",
      "    `preprocessing` is a python library designed for the purpose of preprocessing MRI\n",
      "    data at QTIM. It currently supports reorganization of DICOM and NIfTI files to follow\n",
      "    a BIDS inspired convention, DICOM to NIfTI conversion, and preprocessing for brain data. Its\n",
      "    outputs are also intended to follow a BIDS inspired organizational scheme.\n",
      "    \n",
      "    Public Packages\n",
      "    ---------------\n",
      "    data\n",
      "        The `data` package contains tools for organizing DICOM and NIfTI datasets into\n",
      "        a BIDS inspired organizational scheme and converting files from DICOM to NIfTI format.\n",
      "    \n",
      "    brain\n",
      "        The `brain` package contains tools specific to preprocessing brain data, such as\n",
      "        skullstripping and the brain preprocessing pipeline.\n",
      "    \n",
      "    qc\n",
      "        The `qc` package contains tools related to data quality control.\n",
      "    \n",
      "    Public Modules\n",
      "    --------------\n",
      "    constants\n",
      "        The `constants` module contains important constants that are referenced frequently\n",
      "        throughout the rest of the library.\n",
      "    \n",
      "    dcm_tools\n",
      "        The `dcm_tools` module contains code relevant for analyzing DICOM files.\n",
      "    \n",
      "    synthmorph\n",
      "        The `synthmorph` module uses the Synthmorph models to perform image registration.\n",
      "    \n",
      "    utils\n",
      "        The `utils` module contains custom exceptions and useful functions that are referenced\n",
      "        frequently throughout the rest of the library.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    brain (package)\n",
      "    constants\n",
      "    data (package)\n",
      "    dcm_tools\n",
      "    qc (package)\n",
      "    slurm_concurrency\n",
      "    synthmorph\n",
      "    utils\n",
      "\n",
      "DATA\n",
      "    __all__ = ['data', 'brain', 'qc', 'constants', 'dcm_tools', 'synthmorp...\n",
      "\n",
      "FILE\n",
      "    /autofs/space/crater_001/tools/repos/preprocessing_dev/preprocessing/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "help(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389641a4-86a9-49e7-9d61-7210728f9cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _Helper in module _sitebuiltins object:\n",
      "\n",
      "class _Helper(builtins.object)\n",
      " |  Define the builtin 'help'.\n",
      " |  \n",
      " |  This is a wrapper around pydoc.help that provides a helpful message\n",
      " |  when 'help' is typed at the Python interactive prompt.\n",
      " |  \n",
      " |  Calling help() at the Python prompt starts an interactive help session.\n",
      " |  Calling help(thing) prints help for the python object 'thing'.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, **kwds)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362b053-ea66-42c8-935d-10275de47898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (preprocessing_dev)",
   "language": "python",
   "name": "preprocessing_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
