{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3300ee-1d09-4a02-87a2-8ddb112b51c6",
   "metadata": {},
   "source": [
    "# Installation Guide\n",
    "\n",
    "This guide illustrates the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac6675-1dd1-4d9e-a593-e58572ce980b",
   "metadata": {},
   "source": [
    "## Local Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807a6fb-cef0-463f-a651-f7732ab6d6e7",
   "metadata": {},
   "source": [
    "### External Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60552621-5536-4e45-bd70-75512f484208",
   "metadata": {},
   "source": [
    "#### dcm2niix\n",
    "This tool is required for the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfe1a2-1954-4c51-a1f9-ddaab9eab7af",
   "metadata": {},
   "source": [
    "### model weights\n",
    "In order to complete skullstripping and registration tasks, `preprocessing` relies on [SynthStrip](https://surfer.nmr.mgh.harvard.edu/docs/synthstrip/) and [SynthMorph](https://martinos.org/malte/synthmorph/). The first time a command that requires these models is called, you will be prompted to define an environment variable `PREPROCESSING_MODELS_PATH` and to update your RC file to be used in the future. If you are on a QTIM machine, these models are already downloaded and available if you specify 'QTIM'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2a1a7-5510-4b78-ba0b-edf3d983fe97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### pip\n",
    "\n",
    "From the repo root, run \n",
    "```bash\n",
    "pip install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f8ff8-3be0-4afc-9eb3-4ba3d07298d1",
   "metadata": {},
   "source": [
    "### poetry\n",
    "\n",
    "Within the repository root directory, a poetry.lock file is already present. To install `preprocessing`, simply navigate to this directory, activate the desired virtual environment, and run\n",
    "\n",
    "```bash\n",
    "poetry install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7bd292-2fcd-4e37-80cf-e61a9504898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: \n",
      "preprocessing <command> [<args>]\n",
      "\n",
      "The following commands are available:\n",
      "    validate-installation       Check that the `preprocessing` library is installed correctly along\n",
      "                                with all of its dependencies.\n",
      "\n",
      "    dicom-dataset               Create a DICOM dataset CSV compatible with subsequent `preprocessing`\n",
      "                                scripts. The final CSV provides a series level summary of the location\n",
      "                                of each series alongside metadata extracted from DICOM headers.  If the\n",
      "                                previous organization schems of the dataset does not enforce a DICOM\n",
      "                                series being isolated to a unique directory (instances belonging to\n",
      "                                multiple series must not share the same lowest level directory),\n",
      "                                reorganization must be applied for NIfTI conversion.\n",
      "\n",
      "    nifti-dataset               Create a NIfTI dataset CSV compatible with subsequent `preprocessing`\n",
      "                                scripts. The final CSV provides a series level summary of the location\n",
      "                                of each series alongside metadata generated to simulate DICOM headers.\n",
      "                                Specifically, ['PatientID', 'StudyDate', 'SeriesInstanceUID',\n",
      "                                'SeriesDescription', 'StudyInstanceUID'] (and optionally\n",
      "                                'NormalizedSeriesDescription') are inferred or randomly generated.\n",
      "\n",
      "    dataset-to-nifti            Convert DICOMs to NIfTI file format. A CSV is required to map a\n",
      "                                DICOM series to the resulting .nii.gz file and to provide\n",
      "                                the context for filenames. The outputs will follow a BIDS inspired\n",
      "                                convention.\n",
      "\n",
      "    brain-preprocessing         Preprocess NIfTI files for deep learning. A CSV is required to\n",
      "                                indicate the location of source files and to procide the context\n",
      "                                for filenames. The outputs will follow a BIDS inspired convention.\n",
      "\n",
      "    track-tumors                Longitudinal tracking of individual tumors. Each connected component\n",
      "                                for a given label within a segmentation mask is assigned a unique ID\n",
      "                                that will remain consistent across all scans belonging to the same\n",
      "                                patient. This command assumes that longitudinal or atlas registration\n",
      "                                was used when preprocessing the data.\n",
      "\n",
      "Run `preprocessing <command> --help` for more details about how to use each individual command.\n",
      "\n",
      "positional arguments:\n",
      "  {validate-installation,dicom-dataset,nifti-dataset,dataset-to-nifti,brain-preprocessing,track-tumors,track-volume}\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "/autofs/space/crater_001/tools/repos/preprocessing_dev/notebooks\n"
     ]
    }
   ],
   "source": [
    "!preprocessing --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61df60e-d7e2-4a80-94a3-83c6696e3d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`preprocessing` installation is valid.\n"
     ]
    }
   ],
   "source": [
    "!preprocessing validate-installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ec779-e166-475c-b73e-da29a6824a3e",
   "metadata": {},
   "source": [
    "## docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8199ebd1-f554-4a24-9472-1dd8771e03f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: \n",
      "preprocessing-docker <command> [<args>]\n",
      "\n",
      "The following commands are available:\n",
      "    validate-installation       Check that the `preprocessing` library is installed correctly along\n",
      "                                with all of its dependencies.\n",
      "\n",
      "    dicom-dataset               Create a DICOM dataset CSV compatible with subsequent `preprocessing`\n",
      "                                scripts. The final CSV provides a series level summary of the location\n",
      "                                of each series alongside metadata extracted from DICOM headers.  If the\n",
      "                                previous organization schems of the dataset does not enforce a DICOM\n",
      "                                series being isolated to a unique directory (instances belonging to\n",
      "                                multiple series must not share the same lowest level directory),\n",
      "                                reorganization must be applied for NIfTI conversion.\n",
      "\n",
      "    nifti-dataset               Create a NIfTI dataset CSV compatible with subsequent `preprocessing`\n",
      "                                scripts. The final CSV provides a series level summary of the location\n",
      "                                of each series alongside metadata generated to simulate DICOM headers.\n",
      "                                Specifically, ['PatientID', 'StudyDate', 'SeriesInstanceUID',\n",
      "                                'SeriesDescription', 'StudyInstanceUID'] (and optionally\n",
      "                                'NormalizedSeriesDescription') are inferred or randomly generated.\n",
      "\n",
      "    dataset-to-nifti            Convert DICOMs to NIfTI file format. A CSV is required to map a\n",
      "                                DICOM series to the resulting .nii.gz file and to provide\n",
      "                                the context for filenames. The outputs will follow a BIDS inspired\n",
      "                                convention.\n",
      "\n",
      "    brain-preprocessing         Preprocess NIfTI files for deep learning. A CSV is required to\n",
      "                                indicate the location of source files and to procide the context\n",
      "                                for filenames. The outputs will follow a BIDS inspired convention.\n",
      "\n",
      "    track-tumors                Longitudinal tracking of individual tumors. Each connected component\n",
      "                                for a given label within a segmentation mask is assigned a unique ID\n",
      "                                that will remain consistent across all scans belonging to the same\n",
      "                                patient. This command assumes that longitudinal or atlas registration\n",
      "                                was used when preprocessing the data.\n",
      "\n",
      "Run `preprocessing-docker <command> --help` for more details about how to use each individual command.\n",
      "\n",
      "positional arguments:\n",
      "  {validate-installation,dicom-dataset,nifti-dataset,dataset-to-nifti,brain-preprocessing,track-tumors,track-volume}\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!preprocessing-docker --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba168fd2-e757-40dd-b8b9-f10816d54413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Launching docker with the following command:\n",
      "\n",
      "docker run --rm  qtim/preprocessing:0.1.0 preprocessing validate-installation\n",
      "\n",
      "\n",
      "`preprocessing` installation is valid.\n"
     ]
    }
   ],
   "source": [
    "!preprocessing-docker validate-installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeabbf7c-92cb-4246-b03e-11c95ae70c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389641a4-86a9-49e7-9d61-7210728f9cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (preprocessing_dev)",
   "language": "python",
   "name": "preprocessing_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
